{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing the library for the API and setting the basin of interest (N Atlantic - includes the gulf coast area)\n",
    "\n",
    "setting the list of cities we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting to read in HURDAT2 data\n",
      "--> Completed reading in HURDAT2 data (1.48 seconds)\n",
      "--> Starting to interpolate storms\n",
      "--> Completed interpolating storms (5.8 seconds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AL011860': np.float64(38.3),\n",
       " 'AL051869': np.float64(32.9),\n",
       " 'AL131887': np.float64(42.6),\n",
       " 'AL041892': np.float64(17.5),\n",
       " 'AL081893': np.float64(29.9),\n",
       " 'AL011914': np.float64(21.0),\n",
       " 'AL061915': np.float64(14.7),\n",
       " 'AL041936': np.float64(12.4),\n",
       " 'AL091936': np.float64(39.4),\n",
       " 'AL041939': np.float64(39.6),\n",
       " 'AL041947': np.float64(11.6),\n",
       " 'AL051948': np.float64(8.9),\n",
       " 'AL011955': np.float64(32.8),\n",
       " 'AL051955': np.float64(22.2),\n",
       " 'AL051971': np.float64(9.6),\n",
       " 'AL111971': np.float64(15.2),\n",
       " 'AL061975': np.float64(24.7),\n",
       " 'AL181975': np.float64(8.8),\n",
       " 'AL151977': np.float64(42.7),\n",
       " 'AL041979': np.float64(33.5),\n",
       " 'AL021988': np.float64(3.6),\n",
       " 'AL071988': np.float64(9.6),\n",
       " 'AL012001': np.float64(30.4),\n",
       " 'AL022002': np.float64(19.8),\n",
       " 'AL102002': np.float64(14.9),\n",
       " 'AL032003': np.float64(43.7),\n",
       " 'AL032005': np.float64(28.7),\n",
       " 'AL122005': np.float64(45.9),\n",
       " 'AL032020': np.float64(5.5),\n",
       " 'AL282020': np.float64(10.6),\n",
       " 'AL032021': np.float64(29.0)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tropycal import tracks\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import math\n",
    "#set area of interest\n",
    "basin = tracks.TrackDataset(basin='north_atlantic',include_btk=False)\n",
    "\n",
    "# the gulf coast cities we were given\n",
    "target_cities = {\n",
    "    \"New Orleans, USA\": (29.9511, -90.0715),\n",
    "    \"Houston, USA\": (29.7604, -95.3698),\n",
    "    \"Tampa, USA\": (27.9506, -82.4572),\n",
    "    \"Miami, USA\": (25.7617, -80.1918),\n",
    "    \"Corpus Christi, USA\": (27.8006, -97.3964),\n",
    "    \"Pensacola, USA\": (30.4213, -87.2169),\n",
    "    \"Mobile, USA\": (30.6954, -88.0399),\n",
    "    \"Galveston, USA\": (29.3013, -94.7977),\n",
    "    \"Biloxi, USA\": (30.3960, -88.8853),\n",
    "    \"Key West, USA\": (24.5551, -81.7800),\n",
    "    \"Veracruz, Mexico\": (19.1738, -96.1342),\n",
    "    \"Tampico, Mexico\": (22.2553, -97.8686),\n",
    "    \"Campeche, Mexico\": (19.8453, -90.5235),\n",
    "    \"Cancún, Mexico\": (21.1619, -86.8515),\n",
    "    \"Mérida, Mexico\": (20.9674, -89.5926),\n",
    "    \"Ciudad del Carmen, Mexico\": (18.6491, -91.8071),\n",
    "    \"Progreso, Mexico\": (21.2836, -89.6645),\n",
    "    \"Coatzacoalcos, Mexico\": (18.1489, -94.4202),\n",
    "    \"Tuxpan, Mexico\": (20.9589, -97.4044),\n",
    "    \"Havana, Cuba\": (23.1136, -82.3666),\n",
    "    \"Varadero, Cuba\": (23.1547, -81.2546),\n",
    "    \"Cienfuegos, Cuba\": (22.1613, -80.4490),\n",
    "    \"Belize City, Belize\": (17.5046, -88.1962),\n",
    "    \"George Town, Cayman Islands\": (19.2869, -81.3674),\n",
    "    \"Nassau, Bahamas\": (25.0343, -77.3963)\n",
    "}\n",
    "\n",
    "just_coords = [\n",
    "    (29.9511, -90.0715),\n",
    "    (29.7604, -95.3698),\n",
    "    (27.9506, -82.4572),\n",
    "    (25.7617, -80.1918),\n",
    "    (27.8006, -97.3964),\n",
    "    (30.4213, -87.2169),\n",
    "    (30.6954, -88.0399),\n",
    "    (29.3013, -94.7977),\n",
    "    (30.3960, -88.8853),\n",
    "    (24.5551, -81.7800),\n",
    "    (19.1738, -96.1342),\n",
    "    (22.2553, -97.8686),\n",
    "    (19.8453, -90.5235),\n",
    "    (21.1619, -86.8515),\n",
    "    (20.9674, -89.5926),\n",
    "    (18.6491, -91.8071),\n",
    "    (21.2836, -89.6645),\n",
    "    (18.1489, -94.4202),\n",
    "    (20.9589, -97.4044),\n",
    "    (23.1136, -82.3666),\n",
    "    (23.1547, -81.2546),\n",
    "    (22.1613, -80.4490),\n",
    "    (17.5046, -88.1962),\n",
    "    (19.2869, -81.3674),\n",
    "    (25.0343, -77.3963)\n",
    "]\n",
    "\n",
    "currentPoint = target_cities[\"New Orleans, USA\"]\n",
    "\n",
    "basin.analogs_from_point((currentPoint[0], currentPoint[1]),radius=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: Visualize storm tracks over the last 25-year period for the Gulf Coast region.\n",
    "\n",
    "### For each city of interest, pull all storms of at least Cat1 that pass within the set radius (currently 50km) within the defined 30yr period\n",
    "\n",
    "### Print the IDs for each storm and plot their courses and dump those plots into the specified folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "output_folder = \"/Users/andrew/COSC-3337/hurricanePlots\"  # Change this to your desired folder path\n",
    "# output_folder = \"C:\\Users\\Andrew\\Documents\\COSC-3337\\hurricanePlots\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "yearRange = (1998,2023)\n",
    "radius = 60 #km\n",
    "# stormLog = {}\n",
    "\n",
    "# plot each city and its storms on a separate map (to make easier to read)\n",
    "for city,coords in target_cities.items():\n",
    "    print(\"{} is at {}\".format(city, coords))\n",
    "\n",
    "    # Set the full path for the plot image file\n",
    "    savePathCity = city.split(\",\")\n",
    "    save_path = os.path.join(output_folder, savePathCity[0]+\".png\")\n",
    "\n",
    "    storms = basin.analogs_from_point(coords,radius=radius,year_range=yearRange,thresh={'v_min':33})\n",
    "    \n",
    "    if not storms:\n",
    "        print(\"No hurricanes hit within 50km of {} between 1990 and 2020\\n\".format(savePathCity[0]))\n",
    "    else:\n",
    "        print(\"Storm log for {}: {}\\n\".format(savePathCity[0], storms))\n",
    "        #thresh is min sustained wind of 33kt => 62 kmph\n",
    "        basin.plot_analogs_from_point(coords,radius=radius,domain='north_atlantic', prop={'plot_names':True},year_range=yearRange,thresh={'v_min':33},save_path=save_path)\n",
    "\n",
    "\n",
    "\n",
    "#plot all hurricanes (at least tropical storm) that hit any of the cities we care about in the last 25 years\n",
    "#thresh is min sustained wind of 33kt => 62 kmph\n",
    "\n",
    "#This is the list of storms that meet the criteria we're going to be plotting in the next line\n",
    "storms = basin.analogs_from_shape(just_coords,year_range=yearRange,thresh={'v_min':33})\n",
    "print(storms)\n",
    "print(len(storms))\n",
    "\n",
    "#domain is used to set the focus of the plot. would probably help to set a custom domain since none of the presets are the gulf coast area\n",
    "\n",
    "save_path = os.path.join(output_folder, \"gulf_coast_region.png\")\n",
    "basin.plot_storms(storms,domain='north_atlantic', prop={'plot_names':True},save_path=save_path) #dots\n",
    "\n",
    "# basin.plot_storms(storms[:10],domain='north_atlantic', prop={'dots':False,'linecolor':'category','linewidth':2,'plot_names':True},save_path=save_path) #lines\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(coord1, coord2):\n",
    "    # Calculate the great-circle distance between two points using the Haversine formula\n",
    "    R = 6371  # Radius of Earth in kilometers\n",
    "    lat1, lon1 = coord1\n",
    "    lat2, lon2 = coord2\n",
    "    \n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    \n",
    "    a = math.sin(dlat / 2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "def find_closest_coords(data_coords, just_coords):\n",
    "    # Initialize variables to store the closest coordinate pair and the smallest distance found\n",
    "    closest_data_coord = None\n",
    "    closest_just_coord = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    # Loop through each coordinate in the dictionary's coordinates\n",
    "    for data_coord in data_coords:\n",
    "        # Compare it with each coordinate in just_coords\n",
    "        for ref_coord in just_coords:\n",
    "            # Calculate the distance between the two coordinates\n",
    "            distance = haversine_distance(data_coord, ref_coord)\n",
    "            \n",
    "            # Update the closest coordinate if a shorter distance is found\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_data_coord = data_coord\n",
    "                closest_just_coord = ref_coord\n",
    "    \n",
    "    return closest_data_coord, closest_just_coord, min_distance\n",
    "\n",
    "def find_closest_city(closest_just_coord, target_cities):\n",
    "    closest_city = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    for city, coord in target_cities.items():\n",
    "        # Calculate distance to each city's coordinates\n",
    "        distance = haversine_distance(closest_just_coord, coord)\n",
    "        \n",
    "        # Update the closest city if a shorter distance is found\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_city = city\n",
    "    \n",
    "    return closest_city\n",
    "\n",
    "def append_hurricane_summary_to_csv(data, filename='hurricane_summary.csv'):\n",
    "    # Define the critical fields to include in the CSV\n",
    "    critical_fields = ['id', 'name', 'year', 'lat', 'lon', 'vmax', 'mslp']\n",
    "    \n",
    "    # Extract values for each critical field except time\n",
    "    data_to_write = {field: data[field] for field in critical_fields if field in data}\n",
    "    \n",
    "    # Get start_time and end_time from the time list\n",
    "    start_time = data['time'][0].isoformat() if isinstance(data['time'][0], datetime) else data['time'][0]\n",
    "    end_time = data['time'][-1].isoformat() if isinstance(data['time'][-1], datetime) else data['time'][-1]\n",
    "\n",
    "    # Find the closest coordinate in the data to any coordinate in just_coords\n",
    "    data_coords = list(zip(data['lat'], data['lon']))\n",
    "    closest_data_coord, closest_just_coord, min_distance = find_closest_coords(data_coords, just_coords)\n",
    "    affected_city = find_closest_city(closest_just_coord, target_cities)\n",
    "    \n",
    "    # Check if file exists to decide if headers are needed\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    # Write data to CSV\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # If the file is new, add headers\n",
    "        if not file_exists:\n",
    "            headers = ['id', 'name', 'year', 'start_time', 'end_time', 'init_lat', 'init_lon', 'vmax', 'mslp','impact_lat','impact_lon','city_impacted']\n",
    "            writer.writerow(headers)\n",
    "        \n",
    "        # Prepare a row with the required data\n",
    "        row = [\n",
    "            data['id'],\n",
    "            data['name'],\n",
    "            data['year'],\n",
    "            start_time,\n",
    "            end_time,\n",
    "            data['lat'][0],   # Using the initial latitude\n",
    "            data['lon'][0],   # Using the initial longitude\n",
    "            max(data['vmax']), # Maximum vmax during the period\n",
    "            min(data['mslp']),  # Minimum mslp during the period\n",
    "            closest_data_coord[0],\n",
    "            closest_data_coord[1],\n",
    "            affected_city\n",
    "        ]\n",
    "        \n",
    "        writer.writerow(row)\n",
    "\n",
    "storms = basin.analogs_from_shape(just_coords,year_range=yearRange,thresh={'v_min':33})\n",
    "\n",
    "\n",
    "logOfStormDicts = []\n",
    "#clear csv file before writing everything to it\n",
    "with open('hurricane_summary.csv', 'w') as file:\n",
    "    pass\n",
    "\n",
    "#write each storm to the csv file, writes the headers the first time and just appends every time after\n",
    "for eachStorm in storms:\n",
    "    stormOfInterest = basin.get_storm(eachStorm)\n",
    "    # print(stormOfInterest.to_dict())\n",
    "    logOfStormDicts.append(stormOfInterest.to_dict()) #prob dont need\n",
    "    \n",
    "    append_hurricane_summary_to_csv(stormOfInterest.to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B: Identify Common Patterns and Trends\n",
    "\n",
    "# C: Make a report after performing statistical analysis of track frequency, intensity, motion vectors and duration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
