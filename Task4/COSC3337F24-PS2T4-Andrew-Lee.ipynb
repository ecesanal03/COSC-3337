{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATE / nominal / Each record has a date starting from 01/01/2021 to 12/31/2021\n",
    "\n",
    "CLOUDCOVER / categorical / %/ 17 different types of cloud cover. Categories are: \n",
    "    “Fair”, “Fair / Windy\", \"Partly Cloudy\", \"Partly Cloudy / Windy\", \"Cloudy\", \"Cloudy / Windy\",\"Mostly Cloudy\",\"Mostly Cloudy / Windy\",\"Fog\",\"Haze\", \"Light Rain\",  \"Light Rain with      \n",
    "      Thunder\", \"Thunder\", \"Rain\" \"Thunder / Windy\"  \"Heavy T-Storm\", \"Thunder in the Vicinity\", \"T-Storm\"\n",
    "\n",
    "RAINFALL / continuous / inch / Amount of rainfall of the day/ from 0 to 5\n",
    "\n",
    "MIN TEMP / continuous / Fahrenheit / Minimum temperature at 3pm / from 34 to 83\n",
    "\n",
    "WIND SPEED / continuous / mile per hour / wind speed at 3pm/ from 0 to 29\n",
    "\n",
    "HUMIDITY / continuous / % / Humidity at 3pm/ from 0 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudcover_map = {\n",
    "    \"Fair\": 1, \"Fair / Windy\": 2, \"Partly Cloudy\": 3, \"Partly Cloudy / Windy\": 4,\n",
    "    \"Cloudy\": 5, \"Cloudy / Windy\": 6, \"Mostly Cloudy\": 7, \"Mostly Cloudy / Windy\": 8,\n",
    "    \"Fog\": 9, \"Haze\": 10, \"Light Rain\": 11, \"Light Rain with Thunder\": 12,\n",
    "    \"Thunder\": 13, \"Rain\": 14, \"Thunder / Windy\": 15, \"Heavy T-Storm\": 16,\n",
    "    \"Thunder in the Vicinity\": 17\n",
    "}\n",
    "#reversing map to use in print stmts later on\n",
    "reverse_cloudcover_map = {v: k for k, v in cloudcover_map.items()}\n",
    "\n",
    "#gpt had a neat idea on how to overwrite the cloudcover categories with numerical equivalents (ive never done it with the map method before this is cool)\n",
    "data = pandas.read_csv('weather_data.csv')\n",
    "data[\"cloudcover\"] = data[\"cloudcover\"].map(cloudcover_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is missing enough random inputs throuhgout that I cant just run it through the k-nearest function. I've got two options, either throw out the rows with incomplete fields or use a more advanced version of the k-nearest function to kinda guess an average value and fill in so that the row can be kept. \n",
    "\n",
    "I'll do both just to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for rows with incomplete fields, use best guess and fill empty fields\n",
    "\n",
    "(used this one for the 3 sets required by the assignment doc -- the differences between the two methods wasnt huge and this one is cooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  min_temp  rainfall  windspeed  humidity             cloudcover  \\\n",
      "226   8/15/21         0       4.9          7        52          Mostly Cloudy   \n",
      "103   4/14/21         0       1.2          7        88          Heavy T-Storm   \n",
      "299  10/27/21        65       3.6         21        36  Partly Cloudy / Windy   \n",
      "210   7/30/21        79       2.3         22        52        Thunder / Windy   \n",
      "178   6/28/21        75       4.7          7        87             Light Rain   \n",
      "\n",
      "          OLS  \n",
      "226  0.970020  \n",
      "103  0.775861  \n",
      "299  0.653069  \n",
      "210  0.576821  \n",
      "178  0.563471  \n",
      "Entire dataset sorted by OLS saved to data_w_fill_sorted_by_OLS_k21.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# k = 7\n",
    "# k = 14\n",
    "k = 21  # The golden number for playing around with stuff\n",
    "\n",
    "# Take fields (not date) and scale because euclidean distance is sensitive to large magnitude differences\n",
    "# Ensure cloudcover is encoded properly before scaling (if it is categorical)\n",
    "fields = ['rainfall', 'min_temp', 'windspeed', 'humidity', 'cloudcover']\n",
    "temp = data[fields]\n",
    "\n",
    "# Impute missing values using KNN Imputer\n",
    "imputer = KNNImputer(n_neighbors=5)  # Using k=5 for imputation\n",
    "temp_imputed = imputer.fit_transform(temp)\n",
    "\n",
    "# dataScaled = StandardScaler().fit_transform(temp) # standardscaler transforms data to have a mean of 0 and std dev of 1\n",
    "dataScaled = MinMaxScaler().fit_transform(temp_imputed)  # minmaxscaler normalizes (in theory better for k-nearest)\n",
    "\n",
    "# Use the scikit-learn library to find k-dist instead of manually calculating the k-nearest neighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=k).fit(dataScaled)\n",
    "\n",
    "# Distance is the distance between the current point and k-nearest neighbors\n",
    "# Indices are the indices of the k-nearest neighbors (if you want them later)\n",
    "distances, indices = nbrs.kneighbors(dataScaled)\n",
    "\n",
    "# Calculate the outlier score as the mean distance to k-nearest neighbors and add to data\n",
    "data['OLS'] = np.mean(distances, axis=1)\n",
    "\n",
    "# Sort by OLS to find top outliers\n",
    "top_outliers = data.sort_values(by='OLS', ascending=False)\n",
    "top_outliers['cloudcover'] = top_outliers['cloudcover'].map(reverse_cloudcover_map)\n",
    "\n",
    "# Display top outliers\n",
    "print(top_outliers.head(5))\n",
    "\n",
    "# Save the entire dataset sorted by OLS to a new CSV file\n",
    "# output_file = \"data_w_fill_sorted_by_OLS_k7.csv\"\n",
    "# output_file = \"data_w_fill_sorted_by_OLS_k14.csv\"\n",
    "output_file = \"data_w_fill_sorted_by_OLS_k21.csv\"\n",
    "top_outliers.to_csv(output_file, index=False)\n",
    "print(f\"Entire dataset sorted by OLS saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip rows with incomplete fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  min_temp  rainfall  windspeed  humidity             cloudcover  \\\n",
      "226   8/15/21         0       4.9          7        52          Mostly Cloudy   \n",
      "103   4/14/21         0       1.2          7        88          Heavy T-Storm   \n",
      "299  10/27/21        65       3.6         21        36  Partly Cloudy / Windy   \n",
      "210   7/30/21        79       2.3         22        52        Thunder / Windy   \n",
      "41    2/11/21        40       2.6         17        86                 Cloudy   \n",
      "\n",
      "          OLS  \n",
      "226  0.705377  \n",
      "103  0.508765  \n",
      "299  0.456199  \n",
      "210  0.436678  \n",
      "41   0.364218  \n",
      "Entire dataset sorted by OLS saved to data_no_fill_sorted_by_OLS.csv\n"
     ]
    }
   ],
   "source": [
    "k = 5  # The golden number for playing around with stuff\n",
    "\n",
    "# Drop rows with missing data\n",
    "data_clean = data.dropna(subset=['rainfall', 'min_temp', 'windspeed', 'humidity', 'cloudcover'])\n",
    "\n",
    "# Take fields (not date) and scale the data\n",
    "fields = ['rainfall', 'min_temp', 'windspeed', 'humidity', 'cloudcover']\n",
    "temp = data_clean[fields].values\n",
    "# dataScaled = StandardScaler().fit_transform(temp) # standardscaler transforms data to have a mean of 0 and std dev of 1\n",
    "dataScaled = MinMaxScaler().fit_transform(temp)  # minmaxscaler normalizes (in theory better for k-nearest)\n",
    "\n",
    "\n",
    "# Use a method from the scikit-learn library to find k-dist instead of manually calculating the k-nearest neighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=k).fit(dataScaled)\n",
    "\n",
    "# Distance is the distance between the current point and k-nearest neighbors\n",
    "# Indices are the indices of the k-nearest neighbors (if you want them later)\n",
    "distances, indices = nbrs.kneighbors(dataScaled)\n",
    "\n",
    "# Calculate outlier score as the mean distance to k-nearest neighbors and add to data_clean\n",
    "data_clean.loc[:, 'OLS'] = np.mean(distances, axis=1)\n",
    "\n",
    "# Sort by OLS to find top outliers\n",
    "top_outliers = data_clean.sort_values(by='OLS', ascending=False)\n",
    "top_outliers['cloudcover'] = top_outliers['cloudcover'].map(reverse_cloudcover_map)\n",
    "\n",
    "# Display top outliers\n",
    "print(top_outliers.head(5))\n",
    "\n",
    "output_file = \"data_no_fill_sorted_by_OLS.csv\"\n",
    "top_outliers.to_csv(output_file, index=False)\n",
    "print(f\"Entire dataset sorted by OLS saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
